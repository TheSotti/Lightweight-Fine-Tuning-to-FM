{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8385779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/student/.local/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "/usr/bin/sh: 1: Syntax error: Unterminated quoted string\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/student/.local/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/student/.local/lib/python3.10/site-packages (from wandb) (2.6.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/student/.local/lib/python3.10/site-packages (from wandb) (4.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/student/.local/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: pyyaml in /home/student/.local/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/student/.local/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/student/.local/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/student/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/student/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/student/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets\n",
    "!pip install -q \"datasets==2.15.\n",
    "!pip install scikit-learn\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer,GPT2ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from peft import AutoPeftModelForSequenceClassification,get_peft_model, LoraConfig,TaskType,PeftModel, PeftConfig  # Import LoRA and PEFT helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23434109",
   "metadata": {},
   "source": [
    "# Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b283c",
   "metadata": {},
   "source": [
    "## Load a dataset\n",
    "\n",
    "Using the same criteria applied to selecting the model, we chose the Financial PhraseBank dataset. This dataset contains sentences from financial news labeled with polar sentiments, making it suitable for our sequence classification task. We split the data into 80% for training, 10% for validation, and 10% for testing to ensure a robust evaluation of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48dfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset link in hugginface\n",
    "#https://huggingface.co/datasets/takala/financial_phrasebank\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "\n",
    "# Extract sentences and labels\n",
    "sentences = dataset[\"train\"][\"sentence\"]\n",
    "labels = dataset[\"train\"][\"label\"]\n",
    "\n",
    "# Split the data into training and test sets (80% train, 10% validation, 10% test)\n",
    "#The validation set is used for evaluation during training\n",
    "#The test data is used after training\n",
    "train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "val_sentences, test_sentences, val_labels, test_labels = train_test_split(temp_sentences, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create a dictionary for the datasets\n",
    "train_dataset = {\n",
    "    \"sentence\": train_sentences,\n",
    "    \"label\": train_labels\n",
    "}\n",
    "val_dataset = {\n",
    "    \"sentence\": val_sentences,\n",
    "    \"label\": val_labels\n",
    "}\n",
    "test_dataset = {\n",
    "    \"sentence\": test_sentences,\n",
    "    \"label\": test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbbf4b",
   "metadata": {},
   "source": [
    "## Load a pretrained HF model and preprocess the dataset\n",
    "\n",
    "The chosen model for this project is GPT-2. Since we are using a free virtual environment, Google Colab, this simpler and smaller model is more suitable for applying the LoRA technique. It requires fewer resources and less time for fine-tuning, making it an ideal choice for our setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d96baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add the padding token to the tokenizer (GPT-2 doesn't have a padding token by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set eos_token as pad token for GPT-2\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "pretrained_model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=len(set(labels)))\n",
    "\n",
    "# Crucially, update the model config's pad_token_id\n",
    "pretrained_model.config.pad_token_id = tokenizer.pad_token_id  # Ensure model has the same pad_token_id as tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f4c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887d8b285b8d431db3bbd655f12c6c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f5f973f814577bbd7a7fcbe295091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58159bd43614451699b445b830ac23eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Define a function to tokenize the data\n",
    "def tokenize_data(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_dataset).map(tokenize_data, batched=True)\n",
    "val_dataset = Dataset.from_dict(val_dataset).map(tokenize_data, batched=True)\n",
    "test_dataset = Dataset.from_dict(test_dataset).map(tokenize_data, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9319f37",
   "metadata": {},
   "source": [
    "## Evaluate the pretrained model\n",
    "\n",
    "For the evaluation, we use accuracy and F1 score as the primary metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24cd763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model Metrics: {'accuracy': 0.14537444933920704, 'f1': 0.11116869655351126}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Load data into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Evaluate pre-trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model.to(device)\n",
    "metrics_pre = evaluate_model(pretrained_model, test_loader)\n",
    "print(\"Pre-trained Model Metrics:\", metrics_pre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec544f1",
   "metadata": {},
   "source": [
    "# Perform Lightweight Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d28bd",
   "metadata": {},
   "source": [
    "## Create a PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787af2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank for LoRA matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],  # Layers where LoRA will be applied\n",
    "    task_type=TaskType.SEQ_CLS,  # Task type: Sequence Classification\n",
    ")\n",
    "\n",
    "# Apply LoRA to the pre-trained model\n",
    "model_with_lora = get_peft_model(pretrained_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2a8de",
   "metadata": {},
   "source": [
    "## Train the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee86727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 446,976 || all params: 124,886,784 || trainable%: 0.3579049645477299\n"
     ]
    }
   ],
   "source": [
    "model_with_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f9b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malsotti\u001b[0m (\u001b[33malsotti-udacity\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/student/.local/lib/python3.10/site-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250107_120357-vzilogz6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alsotti-udacity/huggingface/runs/vzilogz6' target=\"_blank\">Parameter-efficient fine-tuning</a></strong> to <a href='https://wandb.ai/alsotti-udacity/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alsotti-udacity/huggingface' target=\"_blank\">https://wandb.ai/alsotti-udacity/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alsotti-udacity/huggingface/runs/vzilogz6' target=\"_blank\">https://wandb.ai/alsotti-udacity/huggingface/runs/vzilogz6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1135' max='1135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1135/1135 03:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.092700</td>\n",
       "      <td>0.623519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.092700</td>\n",
       "      <td>0.588727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.583019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned LoRA Model Metrics: {'accuracy': 0.7533039647577092, 'f1': 0.7128138995456812}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define a function to tokenize the data\n",
    "def tokenize_data(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Load data into DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "output_dir = './results'\n",
    "\n",
    "# Train the model (fine-tuning with LoRA)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,          # Output directory for checkpoints\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate every epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    num_train_epochs=5,              # Number of epochs\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    report_to=\"wandb\",                 # Ensure training logs are reported to W&B\n",
    "    run_name=\"Parameter-efficient fine-tuning\"   # Set the name for the W&B run\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_with_lora,           # LoRA model to train\n",
    "    args=training_args,             # Training arguments\n",
    "    train_dataset=train_dataset,    # Training dataset\n",
    "    eval_dataset=val_dataset,       # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "model_with_lora.save_pretrained(\"gpt-lora\")\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_lora.to(device)\n",
    "metrics_pre = evaluate_model(model_with_lora, test_loader)\n",
    "print(\"Fine-tuned LoRA Model Metrics:\", metrics_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcfbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded LoRA Model Metrics: {'accuracy': 0.7533039647577092, 'f1': 0.7128138995456812}\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoPeftModelForSequenceClassification.from_pretrained(\"gpt-lora\",num_labels=3)\n",
    "loaded_model.config.pad_token_id = tokenizer.pad_token_id  # Ensure model has the same pad_token_id as tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model.to(device)\n",
    "\n",
    "metrics_loaded = evaluate_model(loaded_model, test_loader)\n",
    "print(\"Reloaded LoRA Model Metrics:\", metrics_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbde00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
